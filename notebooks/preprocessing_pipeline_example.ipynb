{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the path to the scripts folder\n",
    "sys.path.append('util/')\n",
    "\n",
    "import numpy as np\n",
    "from util.preprocessing import *\n",
    "from util.features_util import *\n",
    "from util.features_info import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(sub_sample=False):\n",
    "    path_x_train = \"data/x_train.csv\"\n",
    "    path_y_train = \"data/y_train.csv\"\n",
    "\n",
    "    features_names = np.genfromtxt(\n",
    "        path_x_train, \n",
    "        delimiter=\",\", \n",
    "        dtype=str,\n",
    "        max_rows=1\n",
    "    )\n",
    "\n",
    "    x_train = np.genfromtxt(\n",
    "        path_x_train, \n",
    "        delimiter=\",\", \n",
    "        skip_header=1\n",
    "    )\n",
    "    \n",
    "    y_train = np.genfromtxt(\n",
    "        path_y_train,\n",
    "        delimiter=\",\",\n",
    "        skip_header=1,\n",
    "        usecols=0\n",
    "    )\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        x_train = x_train[::50]\n",
    "        y_train = y_train[::50]\n",
    "\n",
    "    return x_train, y_train, features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and create the dictionary of features\n",
    "x, y, features = load_train_data()\n",
    "\n",
    "feature_indexes = dict(zip(features, range(len(features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_to_keep = excercise_features = [\"_TOTINDA\", \"METVL11_\", \"METVL21_\", \"MAXVO2_\", \"ACTIN11_\", \"ACTIN21_\", \"PADUR1_\", \"PADUR2_\", \"PAFREQ1_\", \"PAFREQ2_\", \"_MINAC11\", \"_MINAC21\", \"STRFREQ_\", \"PA1MIN_\", \"PAVIG11_\", \"PAVIG21_\", \"PA1VIGM_\", \"_PACAT1\", \"_PAINDX1\", \"_PA150R2\", \"_PA300R2\", \"_PA30021\", \"_PASTRNG\"]\n",
    "x_clean, features_clean, feature_indexes_clean = keep_features(x, fs_to_keep, features, feature_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(x_clean, columns=features_clean)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", square=True)\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stats(data: np.ndarray, feature_index: Dict[str, int]):\n",
    "    for f in feature_index.keys():\n",
    "        print(\"----- {} -----\".format(f))\n",
    "        values, counts = np.unique(data[:,feature_index[f]], return_counts=True)\n",
    "        print(\"\\t(value, counts): {}\".format([(value,count) for value, count in zip(values, counts)]))\n",
    "        print(\"\\tmean: {:.2f}\".format(np.nanmean(data[:,feature_index[f]])))\n",
    "        print(\"\\tmedian: {:.2f}\".format(np.nanmedian(data[:,feature_index[f]])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# example of pipeline applied just to some features: \"_TOTINDA\", \"METVL11_\"\n",
    "fs_test = [\"_TOTINDA\", \"METVL11_\"]\n",
    "x_test, fs_test, feature_indexes_test = keep_features(x, fs_test, features, feature_indexes)\n",
    "\n",
    "print(\"-> Original data\")\n",
    "stats(x_test, feature_indexes_test)\n",
    "\n",
    "# put to nan everything that means nan (nan aliases)\n",
    "x_test = align_nans(x_test, fs_test, feature_indexes_test)\n",
    "print(\"-> NaN aliases resolved\")\n",
    "stats(x_test, feature_indexes_test)\n",
    "\n",
    "\n",
    "# map values\n",
    "x_test = map_values(x_test, fs_test, feature_indexes_test)\n",
    "print(\"-> Mapped values\")\n",
    "stats(x_test, feature_indexes_test)\n",
    "\n",
    "# replace nans\n",
    "x_test = remove_nans(x_test, fs_test, feature_indexes_test)\n",
    "print(\"-> Removed NaNs\")\n",
    "stats(x_test, feature_indexes_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
